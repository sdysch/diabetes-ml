{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b49ad0b-7847-493a-969e-955c8f882a98",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0bab3-37a4-47ac-bb41-a87a1fb36874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003da1a9-e79c-468f-9fd5-c245f559643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e99833-1835-411a-a394-3be14e5ccfc0",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929f16d-4900-4550-8e3b-c6a3f3effc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/SamDysch_glucose_2-5-2022.csv', skiprows=[0])\n",
    "data.index = pd.to_datetime(data['Device Timestamp'], format=\"%d-%m-%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a1d1e-e074-4652-9de7-5c67c8eabe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-historic glucose records\n",
    "data = data[data['Record Type'] == 0]\n",
    "\n",
    "# only keep bg\n",
    "to_keep = [\n",
    "    'Historic Glucose mmol/L',\n",
    "]\n",
    "data = data[to_keep]\n",
    "\n",
    "data = data.rename(columns={'Historic Glucose mmol/L': 'reading'})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9366525-6fc9-4035-aa9c-0d04ae839611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a26dc-f5d7-4db3-8644-836a9aa3520f",
   "metadata": {},
   "source": [
    "# Setup hypo threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e5259-be75-465f-8557-c4b21cfdec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPO_THRESHOLD = 3.9\n",
    "data['is_hypo'] = (data['reading'] < HYPO_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091b822-af1d-477a-8e6e-1b72359882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some time variables\n",
    "data['hour'] = data.index.hour\n",
    "data['day'] = data.index.dayofweek\n",
    "data['month'] = data.index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb38623-76ac-4eb3-979f-d0c1b9c73aff",
   "metadata": {},
   "source": [
    "# OneHotEncode hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49eebf-8525-4d40-8657-87d19172759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, prefix='hour', columns=['hour'])\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1f8f0-a9cc-41d7-920f-2e7d84082a90",
   "metadata": {
    "tags": []
   },
   "source": [
    "# creating a lagged and rolling variables\n",
    "* Was I hypo 15 mins ago? 30 mins ago? Etc\n",
    "* Rolling average of last N readings\n",
    "* Sign of gradient of last N readings:\n",
    "    * I.e., is BG rising, falling, or stable?\n",
    "    \n",
    "## Lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359843e6-8662-4e1a-80cb-d8c445f2bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lags\n",
    "# To ensure that we do not make a lag between periods of sensor non-usage, create a new df with the lagged indices & merge onto original data frame\n",
    "def create_lag(df, lag):\n",
    "    tolerance = 15 * lag\n",
    "    freq = '15min'\n",
    "    print(f'Creating lag of {tolerance} minutes')\n",
    "    lagged_copy = df[['reading']].shift(lag, freq=freq)\n",
    "    lagged_copy.rename(columns={'reading': f'lagged_reading_{lag}'}, inplace=True)\n",
    "    \n",
    "    merged = pd.merge_asof(df, lagged_copy, left_index=True, right_index=True, direction='backward', tolerance=pd.Timedelta(minutes=tolerance))\n",
    "    # merged = pd.merge_asof(copy, lagged_copy, left_index=True, right_index=True, direction='backward')\n",
    "    return merged\n",
    "\n",
    "NLAGS = 8\n",
    "for lag in range(1, NLAGS):\n",
    "    data = create_lag(data, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa784d-f531-49d4-99b7-1e309b79f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of variable calculation, drop the nans\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02a6f1-2e92-498d-be54-dbdda05a3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagged hypo bools\n",
    "for lag in range(1, NLAGS):\n",
    "    data[f'is_lagged_hypo_{lag}'] = (data[f'lagged_reading_{lag}'] < HYPO_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a4d64-7c8b-470d-b92e-f4f8cbe5e0aa",
   "metadata": {},
   "source": [
    "## Rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1b0d0-82af-4ff5-b78d-59841342bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple differences of lags - was reading higher, lower, or stable?\n",
    "for lag in range(2, NLAGS):\n",
    "    data[f'diff_{lag}'] = data['lagged_reading_1'] - data[f'lagged_reading_{lag}']\n",
    "\n",
    "# gradients - how quick is BG changing?\n",
    "interval = 15\n",
    "for lag in range(2, NLAGS):\n",
    "    data[f'rate_{lag}'] = data[f'diff_{lag}'] / (interval * lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a143a-9724-48c4-b8db-2b97979b4800",
   "metadata": {},
   "source": [
    "## train, test, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003d6b3-3208-40e0-80d4-9cb98f4f32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.65\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385638b2-47bf-4e2c-8d9d-0355b3135aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = int(TRAIN_SPLIT * len(data))\n",
    "ival = int(VAL_SPLIT * len(data))\n",
    "itest = int(TEST_SPLIT * len(data))\n",
    "\n",
    "train_data = data.iloc[:itrain]\n",
    "val_data = data.iloc[itrain:itrain + ival]\n",
    "test_data = data.iloc[itrain + ival:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d4ff6-d62a-4bad-9325-29277400b3bc",
   "metadata": {},
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf76ef-979c-41a8-b9ea-ae8f99e6a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_and_diffs = [f'diff_{v}' for v in range(2, NLAGS)]\n",
    "rates_and_diffs.extend([f'rate_{v}' for v in range(2, NLAGS)])\n",
    "\n",
    "# to fairly compare with baseline, drop any historical variables with time delta < 45 mins\n",
    "vars_to_drop = [\n",
    "    'month',\n",
    "    'day',\n",
    "    'reading',\n",
    "    'is_lagged_hypo_1',\n",
    "    'is_lagged_hypo_2',\n",
    "    'lagged_reading_1',\n",
    "    'lagged_reading_2',\n",
    "]\n",
    "vars_to_drop.extend(rates_and_diffs)\n",
    "\n",
    "train_data = train_data.drop(vars_to_drop, axis='columns')\n",
    "val_data = val_data.drop(vars_to_drop, axis='columns')\n",
    "test_data = test_data.drop(vars_to_drop, axis='columns')\n",
    "\n",
    "print(train_data.columns)\n",
    "print(val_data.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a6a60-3195-46f8-b777-79e9e5e7bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_hypo'\n",
    "\n",
    "X_train = train_data.drop([target], axis='columns')\n",
    "y_train = train_data[target]\n",
    "\n",
    "X_val = val_data.drop(target, axis='columns')\n",
    "y_val = val_data[target]\n",
    "\n",
    "X_test = test_data.drop(target, axis='columns')\n",
    "y_test = test_data[target]\n",
    "\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87c384-7795-44c1-9037-0f2675e5240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine train = train + validation for cross validation\n",
    "X_train = pd.concat([X_train, X_val])\n",
    "y_train = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b607481-d365-4fe3-84c7-83b764902192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "xgb_weight = float(y_train[y_train == 0].count()) / y_train[y_train == 1].count()\n",
    "print(xgb_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda5677-29f3-4d22-9cd2-67101a81d6f8",
   "metadata": {},
   "source": [
    "# objective function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5fc2d-10f2-4f3d-8ae9-745091285afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # hyperparameter space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    eta = trial.suggest_categorical('eta', [0.1, 0.15, 0.2, 0.3])\n",
    "    gamma = trial.suggest_int('gamma', 0, 5)\n",
    "\n",
    "    \n",
    "    # define model\n",
    "    model = XGBClassifier(\n",
    "        #verbosity=2,\n",
    "        n_estimators=n_estimators,\n",
    "        eta=eta,\n",
    "        gamma=gamma,\n",
    "        max_depth=max_depth,\n",
    "        reg_lambda=1,\n",
    "        reg_alpha=0,\n",
    "        subsample=0.5,\n",
    "        scale_pos_weight=xgb_weight,\n",
    "        objective='binary:logistic'   \n",
    "    )\n",
    "    \n",
    "    # fit and evaluate model\n",
    "    ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    splits = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=splits, scoring=ftwo_scorer)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae40de-68c6-4f5d-9aa8-b2a52306d336",
   "metadata": {},
   "source": [
    "# run optuna trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e80ec-fbe8-4413-b692-4d38d88ad66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32b40d-73b5-44fb-b927-3100dd2df3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Best trial: {study.best_trial}')\n",
    "print(f'Best value: {study.best_value}')\n",
    "print(f'Best parameters: {study.best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd04a5-1e62-4b11-8987-2014bb62b227",
   "metadata": {},
   "source": [
    "# fitting optimised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23baca-7122-4b63-804c-f326c5280004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(\n",
    "    n_estimators=108,\n",
    "    eta=0.15,\n",
    "    gamma=0,\n",
    "    max_depth=4,\n",
    "    reg_lambda=1,\n",
    "    reg_alpha=0,\n",
    "    subsample=0.5,\n",
    "    objective='binary:logistic'   \n",
    ")\n",
    "\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_metric=['aucpr', 'logloss'],\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    #verbose=True\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7f9d7-803c-4eaa-9b85-7568aa0c38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evals_result()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "ax[0].plot(results['validation_0']['aucpr'], label='Train')\n",
    "ax[0].plot(results['validation_1']['aucpr'], label='Test')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_ylabel('AUC (PR)')\n",
    "\n",
    "ax[1].plot(results['validation_0']['logloss'], label='Train')\n",
    "ax[1].plot(results['validation_1']['logloss'], label='Test')\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].set_ylabel('logloss')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ad130-26b1-47a5-aa1c-5c28a9c65d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, fbeta_score\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred)}')\n",
    "print(f'F2: {fbeta_score(y_test, y_pred, beta=2)}')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "sns.heatmap(cm, annot=True, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abdaf5-a6c5-410a-9c87-a36774b04ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "# precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(fpr, tpr, 'b')\n",
    "ax[0].plot([0, 1], [0, 1], 'r--')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "\n",
    "ax[1].plot(recall, precision, 'b')\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "\n",
    "plt.show()\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46ecc8-f8eb-4734-afeb-f67e74592ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
