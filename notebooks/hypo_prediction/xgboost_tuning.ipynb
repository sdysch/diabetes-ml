{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b49ad0b-7847-493a-969e-955c8f882a98",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a0bab3-37a4-47ac-bb41-a87a1fb36874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003da1a9-e79c-468f-9fd5-c245f559643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e99833-1835-411a-a394-3be14e5ccfc0",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d929f16d-4900-4550-8e3b-c6a3f3effc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdysch/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/SamDysch_glucose_2-5-2022.csv', skiprows=[0])\n",
    "data.index = pd.to_datetime(data['Device Timestamp'], format=\"%d-%m-%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213a1d1e-e074-4652-9de7-5c67c8eabe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Device Timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-09 19:04:00</th>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09 19:20:00</th>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09 19:35:00</th>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09 19:50:00</th>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09 20:05:00</th>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     reading\n",
       "Device Timestamp            \n",
       "2019-12-09 19:04:00      6.8\n",
       "2019-12-09 19:20:00      7.6\n",
       "2019-12-09 19:35:00      7.7\n",
       "2019-12-09 19:50:00      7.2\n",
       "2019-12-09 20:05:00      5.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop non-historic glucose records\n",
    "data = data[data['Record Type'] == 0]\n",
    "\n",
    "# only keep bg\n",
    "to_keep = [\n",
    "    'Historic Glucose mmol/L',\n",
    "]\n",
    "data = data[to_keep]\n",
    "\n",
    "data = data.rename(columns={'Historic Glucose mmol/L': 'reading'})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9366525-6fc9-4035-aa9c-0d04ae839611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a26dc-f5d7-4db3-8644-836a9aa3520f",
   "metadata": {},
   "source": [
    "# Setup hypo threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3e5259-be75-465f-8557-c4b21cfdec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPO_THRESHOLD = 3.9\n",
    "data['is_hypo'] = (data['reading'] < HYPO_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c091b822-af1d-477a-8e6e-1b72359882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some time variables\n",
    "data['hour'] = data.index.hour\n",
    "data['day'] = data.index.dayofweek\n",
    "data['month'] = data.index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb38623-76ac-4eb3-979f-d0c1b9c73aff",
   "metadata": {},
   "source": [
    "# OneHotEncode hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f49eebf-8525-4d40-8657-87d19172759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['reading', 'is_hypo', 'day', 'month', 'hour_0', 'hour_1', 'hour_2',\n",
      "       'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9',\n",
      "       'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15',\n",
      "       'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21',\n",
      "       'hour_22', 'hour_23'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.get_dummies(data, prefix='hour', columns=['hour'])\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1f8f0-a9cc-41d7-920f-2e7d84082a90",
   "metadata": {
    "tags": []
   },
   "source": [
    "# creating a lagged and rolling variables\n",
    "* Was I hypo 15 mins ago? 30 mins ago? Etc\n",
    "* Rolling average of last N readings\n",
    "* Sign of gradient of last N readings:\n",
    "    * I.e., is BG rising, falling, or stable?\n",
    "    \n",
    "## Lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359843e6-8662-4e1a-80cb-d8c445f2bb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag of 15 minutes\n",
      "Creating lag of 30 minutes\n",
      "Creating lag of 45 minutes\n",
      "Creating lag of 60 minutes\n",
      "Creating lag of 75 minutes\n",
      "Creating lag of 90 minutes\n",
      "Creating lag of 105 minutes\n"
     ]
    }
   ],
   "source": [
    "# create lags\n",
    "# To ensure that we do not make a lag between periods of sensor non-usage, create a new df with the lagged indices & merge onto original data frame\n",
    "def create_lag(df, lag):\n",
    "    tolerance = 15 * lag\n",
    "    freq = '15min'\n",
    "    print(f'Creating lag of {tolerance} minutes')\n",
    "    lagged_copy = df[['reading']].shift(lag, freq=freq)\n",
    "    lagged_copy.rename(columns={'reading': f'lagged_reading_{lag}'}, inplace=True)\n",
    "    \n",
    "    merged = pd.merge_asof(df, lagged_copy, left_index=True, right_index=True, direction='backward', tolerance=pd.Timedelta(minutes=tolerance))\n",
    "    # merged = pd.merge_asof(copy, lagged_copy, left_index=True, right_index=True, direction='backward')\n",
    "    return merged\n",
    "\n",
    "NLAGS = 8\n",
    "for lag in range(1, NLAGS):\n",
    "    data = create_lag(data, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23fa784d-f531-49d4-99b7-1e309b79f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of variable calculation, drop the nans\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d02a6f1-2e92-498d-be54-dbdda05a3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagged hypo bools\n",
    "for lag in range(1, NLAGS):\n",
    "    data[f'is_lagged_hypo_{lag}'] = (data[f'lagged_reading_{lag}'] < HYPO_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a4d64-7c8b-470d-b92e-f4f8cbe5e0aa",
   "metadata": {},
   "source": [
    "## Rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a1b0d0-82af-4ff5-b78d-59841342bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple differences of lags - was reading higher, lower, or stable?\n",
    "for lag in range(2, NLAGS):\n",
    "    data[f'diff_{lag}'] = data['lagged_reading_1'] - data[f'lagged_reading_{lag}']\n",
    "\n",
    "# gradients - how quick is BG changing?\n",
    "interval = 15\n",
    "for lag in range(2, NLAGS):\n",
    "    data[f'rate_{lag}'] = data[f'diff_{lag}'] / (interval * lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a143a-9724-48c4-b8db-2b97979b4800",
   "metadata": {},
   "source": [
    "## train, test, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f003d6b3-3208-40e0-80d4-9cb98f4f32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.65\n",
    "VAL_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385638b2-47bf-4e2c-8d9d-0355b3135aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = int(TRAIN_SPLIT * len(data))\n",
    "ival = int(VAL_SPLIT * len(data))\n",
    "itest = int(TEST_SPLIT * len(data))\n",
    "\n",
    "train_data = data.iloc[:itrain]\n",
    "val_data = data.iloc[itrain:itrain + ival]\n",
    "test_data = data.iloc[itrain + ival:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d4ff6-d62a-4bad-9325-29277400b3bc",
   "metadata": {},
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32cf76ef-979c-41a8-b9ea-ae8f99e6a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['is_hypo', 'hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5',\n",
      "       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n",
      "       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n",
      "       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23',\n",
      "       'lagged_reading_3', 'lagged_reading_4', 'lagged_reading_5',\n",
      "       'lagged_reading_6', 'lagged_reading_7', 'is_lagged_hypo_3',\n",
      "       'is_lagged_hypo_4', 'is_lagged_hypo_5', 'is_lagged_hypo_6',\n",
      "       'is_lagged_hypo_7'],\n",
      "      dtype='object')\n",
      "Index(['is_hypo', 'hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5',\n",
      "       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n",
      "       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n",
      "       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23',\n",
      "       'lagged_reading_3', 'lagged_reading_4', 'lagged_reading_5',\n",
      "       'lagged_reading_6', 'lagged_reading_7', 'is_lagged_hypo_3',\n",
      "       'is_lagged_hypo_4', 'is_lagged_hypo_5', 'is_lagged_hypo_6',\n",
      "       'is_lagged_hypo_7'],\n",
      "      dtype='object')\n",
      "Index(['is_hypo', 'hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5',\n",
      "       'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n",
      "       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n",
      "       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23',\n",
      "       'lagged_reading_3', 'lagged_reading_4', 'lagged_reading_5',\n",
      "       'lagged_reading_6', 'lagged_reading_7', 'is_lagged_hypo_3',\n",
      "       'is_lagged_hypo_4', 'is_lagged_hypo_5', 'is_lagged_hypo_6',\n",
      "       'is_lagged_hypo_7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "rates_and_diffs = [f'diff_{v}' for v in range(2, NLAGS)]\n",
    "rates_and_diffs.extend([f'rate_{v}' for v in range(2, NLAGS)])\n",
    "\n",
    "# to fairly compare with baseline, drop any historical variables with time delta < 45 mins\n",
    "vars_to_drop = [\n",
    "    'month',\n",
    "    'day',\n",
    "    'reading',\n",
    "    'is_lagged_hypo_1',\n",
    "    'is_lagged_hypo_2',\n",
    "    'lagged_reading_1',\n",
    "    'lagged_reading_2',\n",
    "]\n",
    "vars_to_drop.extend(rates_and_diffs)\n",
    "\n",
    "train_data = train_data.drop(vars_to_drop, axis='columns')\n",
    "val_data = val_data.drop(vars_to_drop, axis='columns')\n",
    "test_data = test_data.drop(vars_to_drop, axis='columns')\n",
    "\n",
    "print(train_data.columns)\n",
    "print(val_data.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718a6a60-3195-46f8-b777-79e9e5e7bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hour_0', 'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6',\n",
      "       'hour_7', 'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12',\n",
      "       'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18',\n",
      "       'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23',\n",
      "       'lagged_reading_3', 'lagged_reading_4', 'lagged_reading_5',\n",
      "       'lagged_reading_6', 'lagged_reading_7', 'is_lagged_hypo_3',\n",
      "       'is_lagged_hypo_4', 'is_lagged_hypo_5', 'is_lagged_hypo_6',\n",
      "       'is_lagged_hypo_7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "target = 'is_hypo'\n",
    "\n",
    "X_train = train_data.drop([target], axis='columns')\n",
    "y_train = train_data[target]\n",
    "\n",
    "X_val = val_data.drop(target, axis='columns')\n",
    "y_val = val_data[target]\n",
    "\n",
    "X_test = test_data.drop(target, axis='columns')\n",
    "y_test = test_data[target]\n",
    "\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee87c384-7795-44c1-9037-0f2675e5240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine train = train + validation for cross validation\n",
    "X_train = pd.concat([X_train, X_val])\n",
    "y_train = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda5677-29f3-4d22-9cd2-67101a81d6f8",
   "metadata": {},
   "source": [
    "# objective function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb5fc2d-10f2-4f3d-8ae9-745091285afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    # hyperparameter space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    eta = trial.suggest_categorical('eta', [0.1, 0.15, 0.2, 0.3])\n",
    "    # gamma = trial.suggest_int('gamma', 0, 5)\n",
    "\n",
    "    \n",
    "    # define model\n",
    "    model = XGBClassifier(\n",
    "        #verbosity=2,\n",
    "        n_estimators=n_estimators,\n",
    "        eta=eta,\n",
    "        gamma=0,\n",
    "        max_depth=max_depth,\n",
    "        reg_lambda=1,\n",
    "        reg_alpha=0,\n",
    "        subsample=0.5,\n",
    "        objective='binary:logistic'   \n",
    "    )\n",
    "    \n",
    "    # fit and evaluate model\n",
    "    ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    splits = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=splits, scoring=ftwo_scorer)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae40de-68c6-4f5d-9aa8-b2a52306d336",
   "metadata": {},
   "source": [
    "# run optuna trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593e80ec-fbe8-4413-b692-4d38d88ad66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-19 11:04:36,207]\u001b[0m A new study created in memory with name: no-name-b9a5f4f3-7c4d-4b32-b969-c28fcf20839b\u001b[0m\n",
      "\u001b[32m[I 2022-06-19 11:04:37,221]\u001b[0m Trial 0 finished with value: 0.2913762094069756 and parameters: {'n_estimators': 29, 'max_depth': 2, 'eta': 0.2}. Best is trial 0 with value: 0.2913762094069756.\u001b[0m\n",
      "\u001b[32m[I 2022-06-19 11:04:40,735]\u001b[0m Trial 1 finished with value: 0.41857048940523855 and parameters: {'n_estimators': 63, 'max_depth': 7, 'eta': 0.3}. Best is trial 1 with value: 0.41857048940523855.\u001b[0m\n",
      "\u001b[32m[I 2022-06-19 11:04:45,162]\u001b[0m Trial 2 finished with value: 0.4190965889234852 and parameters: {'n_estimators': 86, 'max_depth': 6, 'eta': 0.3}. Best is trial 2 with value: 0.4190965889234852.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# optimise\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c32b40d-73b5-44fb-b927-3100dd2df3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value: 0.4190965889234852\n",
      "Best parameters: {'n_estimators': 86, 'max_depth': 6, 'eta': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# print(f'Best trial: {study.best_trial}')\n",
    "print(f'Best value: {study.best_value}')\n",
    "print(f'Best parameters: {study.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23baca-7122-4b63-804c-f326c5280004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
