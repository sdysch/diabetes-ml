{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b49ad0b-7847-493a-969e-955c8f882a98",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0bab3-37a4-47ac-bb41-a87a1fb36874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003da1a9-e79c-468f-9fd5-c245f559643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e99833-1835-411a-a394-3be14e5ccfc0",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929f16d-4900-4550-8e3b-c6a3f3effc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/SamDysch_glucose_2-5-2022.csv', skiprows=[0])\n",
    "data.index = pd.to_datetime(data['Device Timestamp'], format=\"%d-%m-%Y %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a1d1e-e074-4652-9de7-5c67c8eabe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-historic glucose records\n",
    "data = data[data['Record Type'] == 0]\n",
    "\n",
    "# only keep bg\n",
    "to_keep = [\n",
    "    'Historic Glucose mmol/L',\n",
    "]\n",
    "data = data[to_keep]\n",
    "\n",
    "data = data.rename(columns={'Historic Glucose mmol/L': 'reading'})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9366525-6fc9-4035-aa9c-0d04ae839611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a26dc-f5d7-4db3-8644-836a9aa3520f",
   "metadata": {},
   "source": [
    "# Setup hypo threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e5259-be75-465f-8557-c4b21cfdec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPO_THRESHOLD = 3.9\n",
    "data['is_hypo'] = (data['reading'] < HYPO_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091b822-af1d-477a-8e6e-1b72359882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some time variables\n",
    "data['hour'] = data.index.hour\n",
    "data['day'] = data.index.dayofweek\n",
    "data['month'] = data.index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb38623-76ac-4eb3-979f-d0c1b9c73aff",
   "metadata": {},
   "source": [
    "# Encode hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49eebf-8525-4d40-8657-87d19172759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode\n",
    "# data = pd.get_dummies(data, prefix='hour', columns=['hour'])\n",
    "\n",
    "# sin/cosine encode\n",
    "data['sin_hour'] = np.sin(2 * np.pi * data['hour'] / 23)\n",
    "data['cos_hour'] = np.cos(2 * np.pi * data['hour'] / 23)\n",
    "data = data.drop(['hour'], axis='columns')\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1f8f0-a9cc-41d7-920f-2e7d84082a90",
   "metadata": {
    "tags": []
   },
   "source": [
    "# creating a lagged and rolling variables\n",
    "* Was I hypo 15 mins ago? 30 mins ago? Etc\n",
    "* Rolling average of last N readings\n",
    "* Sign of gradient of last N readings:\n",
    "    * I.e., is BG rising, falling, or stable?\n",
    "    \n",
    "## Lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359843e6-8662-4e1a-80cb-d8c445f2bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lags\n",
    "# To ensure that we do not make a lag between periods of sensor non-usage, create a new df with the lagged indices & merge onto original data frame\n",
    "def create_lag(df, lag):\n",
    "    tolerance = 15 * lag\n",
    "    freq = '15min'\n",
    "    print(f'Creating lag of {tolerance} minutes')\n",
    "    lagged_copy = df[['reading']].shift(lag, freq=freq)\n",
    "    lagged_copy.rename(columns={'reading': f'lagged_reading_{lag}'}, inplace=True)\n",
    "    \n",
    "    merged = pd.merge_asof(df, lagged_copy, left_index=True, right_index=True, direction='backward', tolerance=pd.Timedelta(minutes=tolerance))\n",
    "    # merged = pd.merge_asof(copy, lagged_copy, left_index=True, right_index=True, direction='backward')\n",
    "    return merged\n",
    "\n",
    "NLAGS = 8\n",
    "for lag in range(1, NLAGS):\n",
    "    data = create_lag(data, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa784d-f531-49d4-99b7-1e309b79f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of variable calculation, drop the nans\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02a6f1-2e92-498d-be54-dbdda05a3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagged hypo bools\n",
    "for lag in range(1, NLAGS):\n",
    "    data[f'is_lagged_hypo_{lag}'] = (data[f'lagged_reading_{lag}'] < HYPO_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a4d64-7c8b-470d-b92e-f4f8cbe5e0aa",
   "metadata": {},
   "source": [
    "## Rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1b0d0-82af-4ff5-b78d-59841342bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple differences of lags - was reading higher, lower, or stable?\n",
    "for lag in range(2, NLAGS):\n",
    "    data[f'diff_{lag}'] = data['lagged_reading_1'] - data[f'lagged_reading_{lag}']\n",
    "\n",
    "# gradients - how quick is BG changing?\n",
    "interval = 15\n",
    "for lag in range(2, NLAGS):\n",
    "    data[f'rate_{lag}'] = data[f'diff_{lag}'] / (interval * lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a143a-9724-48c4-b8db-2b97979b4800",
   "metadata": {},
   "source": [
    "## train, test, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003d6b3-3208-40e0-80d4-9cb98f4f32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.5\n",
    "VAL_SPLIT = 0.35\n",
    "TEST_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385638b2-47bf-4e2c-8d9d-0355b3135aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain = int(TRAIN_SPLIT * len(data))\n",
    "ival = int(VAL_SPLIT * len(data))\n",
    "itest = int(TEST_SPLIT * len(data))\n",
    "\n",
    "train_data = data.iloc[:itrain]\n",
    "val_data = data.iloc[itrain:itrain + ival]\n",
    "test_data = data.iloc[itrain + ival:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d4ff6-d62a-4bad-9325-29277400b3bc",
   "metadata": {},
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf76ef-979c-41a8-b9ea-ae8f99e6a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_and_diffs = [f'diff_{v}' for v in range(2, NLAGS)]\n",
    "rates_and_diffs.extend([f'rate_{v}' for v in range(2, NLAGS)])\n",
    "\n",
    "# to fairly compare with baseline, drop any historical variables with time delta < 45 mins\n",
    "vars_to_drop = [\n",
    "    'month',\n",
    "    'day',\n",
    "    'reading',\n",
    "    'is_lagged_hypo_1',\n",
    "    'is_lagged_hypo_2',\n",
    "    'lagged_reading_1',\n",
    "    'lagged_reading_2',\n",
    "    \n",
    "    'is_lagged_hypo_3',\n",
    "    'is_lagged_hypo_4',\n",
    "    'is_lagged_hypo_5',\n",
    "    'is_lagged_hypo_6',\n",
    "    'is_lagged_hypo_7',\n",
    "]\n",
    "vars_to_drop.extend(rates_and_diffs)\n",
    "\n",
    "train_data = train_data.drop(vars_to_drop, axis='columns')\n",
    "val_data = val_data.drop(vars_to_drop, axis='columns')\n",
    "test_data = test_data.drop(vars_to_drop, axis='columns')\n",
    "\n",
    "print(train_data.columns)\n",
    "print(val_data.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a6a60-3195-46f8-b777-79e9e5e7bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_hypo'\n",
    "\n",
    "X_train = train_data.drop([target], axis='columns')\n",
    "y_train = train_data[target]\n",
    "\n",
    "X_val = val_data.drop(target, axis='columns')\n",
    "y_val = val_data[target]\n",
    "\n",
    "X_test = test_data.drop(target, axis='columns')\n",
    "y_test = test_data[target]\n",
    "\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdec039-4951-4953-9e52-c2eb30d7aa60",
   "metadata": {},
   "source": [
    "# class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1342a-179f-4b55-8539-07a8b04c4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggestion here is to scale both classes to 50% of total: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "neg = y_train[y_train == 0].count()\n",
    "pos = y_train[y_train == 1].count()\n",
    "total = neg + pos\n",
    "\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "print(weight_for_0, weight_for_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda5677-29f3-4d22-9cd2-67101a81d6f8",
   "metadata": {},
   "source": [
    "# model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5fc2d-10f2-4f3d-8ae9-745091285afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, TruePositives, FalseNegatives, TrueNegatives, FalsePositives, BinaryAccuracy, AUC\n",
    "\n",
    "input_shape = (X_train.shape[1],)\n",
    "activation = 'relu'\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# normalize data\n",
    "norm_layer = Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# preprocessing\n",
    "model.add(norm_layer)\n",
    "\n",
    "# FF part\n",
    "model.add(Dense(40, input_shape=input_shape, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LayerNormalization())\n",
    "# model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "METRICS = [\n",
    "      # TruePositives(name='tp'),\n",
    "      # FalsePositives(name='fp'),\n",
    "      # TrueNegatives(name='tn'),\n",
    "      # FalseNegatives(name='fn'), \n",
    "      # BinaryAccuracy(name='accuracy'),\n",
    "      Precision(name='precision'),\n",
    "      Recall(name='recall'),\n",
    "      # AUC(name='auc'),\n",
    "      AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=METRICS)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86acf5-059f-4e8f-8e4c-29b9c1d88f2a",
   "metadata": {},
   "source": [
    "# fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bed45-e39a-4390-b68e-1ee1ba4c857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a relatively large batch size, to ensure a good chance that positive samples make it into each weight update\n",
    "# batch_size = 128\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "callbacks = None\n",
    "class_weight = {\n",
    "    0: weight_for_0,\n",
    "    1: weight_for_1,\n",
    "}\n",
    "validation_data = (X_val, y_val)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=validation_data,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d2a12-a936-4d25-9106-a67a319364be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curve\n",
    "fig, ax = plt.subplots(1, 3, figsize=(45, 10))\n",
    "\n",
    "ax[0].plot(history.history['loss'], label='train')\n",
    "ax[0].plot(history.history['val_loss'], label='validation')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].plot(history.history['precision'], label='train')\n",
    "ax[1].plot(history.history['val_precision'], label='validation')\n",
    "ax[1].set_ylabel('precision')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "ax[2].plot(history.history['recall'], label='train')\n",
    "ax[2].plot(history.history['val_recall'], label='validation')\n",
    "ax[2].set_ylabel('recall')\n",
    "ax[2].set_xlabel('epoch')\n",
    "ax[2].legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf92fac-03e9-47a8-a340-c7589e11f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, fbeta_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred > 0.5\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred)}')\n",
    "print(f'F2: {fbeta_score(y_test, y_pred, beta=2)}')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='all')\n",
    "sns.heatmap(cm, annot=True, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7450a90-61c8-4ef2-bd78-5a599bcefeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
